
# 《Human-Centered Explainable AI (HCXAI): Beyond Opening the Black-Box of AI》总结
- 2022年
## 一、摘要
AI系统的可解释性对于其 accountability 至关重要，尤其是在医疗、法律等高风险领域。当前可解释AI（XAI）研究过度关注“打开黑箱”，却忽视了“与黑箱交互的主体是谁”这一核心问题，导致难以满足用户需求并加剧了算法不透明性。  
本文基于CHI 2021首届以人为中心的XAI（HCXAI）研讨会的成果，通过多利益相关者的反思性讨论，试图从**概念、方法和技术层面**将HCXAI落地，强调采用**整体性方法**（历史、社会和技术视角），目标是制定可操作的框架、可迁移的评估方法、具体的设计指南，并明确XAI的协调研究议程。

## 二、CCS概念与关键词
- **CCS概念**：以人为本的计算（HCI理论、概念和模型）；计算方法（人工智能的哲学/理论基础）。  
- **关键词**：可解释人工智能、可解释机器学习、可解释性、负责任的AI、对自动化的信任、算法公平性。

## 三、引言
### （一）可解释性的重要性
AI系统在医疗、金融、刑事司法等高风险领域的决策需具备可解释性，以确保用户做出知情且负责任的行动。深度学习模型的复杂性推动了“打开黑箱”技术的发展，但XAI研究长期以算法为中心，忽视了“解释的对象是谁”这一核心问题。  
可解释性不仅是技术问题，更是人机交互（HCI）问题：不同利益相关者（如工程师、用户、政策制定者）对解释的需求和理解差异显著，且算法透明度无法涵盖AI可解释性的社会技术内涵。

### （二）HCXAI研讨会背景
- **首届研讨会成果**：吸引14国超100名参与者，涉及公共政策、机器学习等多领域；收到50+投稿（录用24篇），讨论了XAI系统评估的矛盾、“以人为本”的语境定义、AI问责的设计指南，并促成社区合作及ACM TiiS期刊特刊。  
- **第二届研讨会目标**：为跨学科利益相关者提供交流平台，从概念、方法和技术层面扩展HCXAI讨论，进一步构建支持性社区。

## 四、从XAI到以人为中心的XAI：背景与机遇
### （一）概念澄清与挑战
- **术语模糊性**：“可解释性”“透明度”“可解释性”在不同领域常被混用。XAI社区更关注**事后解释**（向用户传达不透明模型的决策逻辑，而非精确描述模型工作原理），需在不同研究社区间明确概念、定义和评估方法的差异。  
- **用户中心缺失**：现有XAI设计常以开发者为中心，忽视数据科学家、决策者、监管机构等利益相关者的多样化需求，其背景（如专业、教育）会影响对解释的理解和反应。

### （二）解释的多维框架
- **“谁”与“为什么”**：解释需明确对象（如自动驾驶场景中工程师、司机的不同需求）和目的（如可信度、因果关系、公平性）。用户目标（如评估AI能力、调整使用方式）应指导数据收集和解释形式。  
- **“何时”与“何地”**：应用场景和时机影响解释效果。例如，医生在高压力工作中可能无法即时处理AI解释，而在回顾性分析中更需深入理解系统行为。现有研究缺乏对解释时机的关注，需结合时间因素评估解释有效性。  
- **跨学科方法**：借鉴人机交互领域的自动化透明度和信任研究，采用主客观结合的评估方法（如访谈、任务绩效）。现有评估工具（如“解释满意度量表”）需进一步整合“谁、为什么、何时、何地”维度。

### （三）社会技术视角
基于“技术的社会建构”（SCOT）理论，不同社会群体对可解释性的理解存在差异，需系统性地阐明各群体的解释灵活性，从概念、方法和技术角度推动XAI发展，避免规范性强加，而是包容多元视角。

## 五、研讨会目标
1. **深化建设性对话**：围绕HCXAI的落地，讨论利益相关者需求、解释目标的社会影响、跨领域优先级、评估方法的可迁移性等核心问题，避免AI解释被滥用（如诱导过度信任）或“伦理粉饰”。  
2. **构建多元社区**：鼓励全球南方和边缘群体参与，探讨权力动态、地域差异对HCXAI的影响，促进公平对话。  

**引导性问题示例**：  
- 如何梳理XAI中不同利益相关者的解释需求？  
- 如何避免AI解释被“武器化”（如设计暗模式）？  
- 从AI治理角度，如何应对组织中可能导致伤害的不正当激励？。

## 六、研讨会安排
### （一）前期准备
- **宣传**：通过Twitter（超6万关注）、LinkedIn及学术邮件列表（如CHI、NeurIPS）推广，预计吸引50+投稿。  
- **社区建设**：依托Discord平台维持首届参与者社区，鼓励线上互动；更新 workshop 网站（含往届资料），作为资源门户。  
- **评审团队**：招募跨学科程序委员会（PC），确保评审多样性。

### （二）举办形式
- **全虚拟模式**：应对疫情、签证等挑战，降低参与门槛，2021年虚拟模式已吸引全球南方参与者。提供预录播、会议存档等功能，缓解技术不平等问题。  
- **技术平台**：网站（hc xai.jimdosite.com）发布资料，Discord用于实时讨论和异步交流，Zoom进行直播展示（支持字幕功能）。

### （三）日程结构
| **时间**       | **内容**                                                                 |
|----------------|--------------------------------------------------------------------------|
| **会前2周**    | 分发阅读材料，参与者通过Discord自我介绍，建立初步联系                   |
| **第一天（4小时）** | 开幕致辞、主旨演讲（如AI与HCI领域专家）、论文口头报告与海报展示、虚拟社交活动 |
| **第二天（4小时）** | 专家小组讨论、分组研讨（如“未来论文”设计虚构、框架构建）、成果汇报与闭幕 |
| **会后**       | 网站发布会议总结，推动社区持续讨论，筹备期刊特刊和合成论文 |

## 七、组织者与征稿信息
### （一）核心团队
- **Upol Ehsan**（佐治亚理工学院）：研究AI可解释性、全球南方伦理问题，首届HCXAI研讨会负责人。  
- **Philipp Wintersberger**（因戈尔施塔特技术高等学院）：人机交互、自动驾驶信任研究。  
- **Q. Vera Liao**（微软研究院）：人机交互与XAI设计，跨领域活动组织者。  
团队成员覆盖AI、HCI、社会学、公共政策等领域，具备丰富的学术会议组织经验。

### （二）征稿要求
- **主题**：围绕HCXAI的“谁、为什么、何地”，如利益相关者分析、社会因素影响、跨领域应用等。  
- **格式**：符合CHI扩展摘要格式，不超过6页（不含参考文献），通过 workshop 官网提交。  
- **参与要求**：录用论文需至少一名作者参会并注册，鼓励参与小组讨论。

## 八、结论与展望
HCXAI旨在超越“打开黑箱”的技术视角，将人置于XAI的核心，通过跨学科协作解决可解释性的社会技术挑战。未来需构建更包容的评估框架，推动可操作的设计指南，并探索将 workshop 升级为独立会议的可能性（如借鉴FAT*到FAccT的发展路径）。