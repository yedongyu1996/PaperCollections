
# 《A user-centered explainable artificial intelligence approach for financial fraud detection》主要内容与观点总结
- 2023年
## 一、研究背景与目标
### （一）研究背景
1. **AI在金融欺诈检测中的应用与挑战**  
   机器学习（ML）虽显著提升金融欺诈检测的预测性能（如Abbasi等，2012；Bao等，2020），但其黑箱特性导致解释性不足，引发监管机构（如欧盟GDPR）和伦理学界的担忧。  
   - 缺乏解释性成为利益相关者（如监管者、审计师、投资者）应用ML模型的主要障碍。  
2. **可解释AI（XAI）的必要性**  
   XAI旨在平衡模型预测性能与可解释性，帮助用户理解模型逻辑，增强信任并辅助决策（Adadi & Berrada，2018）。  
   - 现有XAI研究多聚焦开发者需求，忽视外部利益相关者（如金融欺诈领域的非技术人员）的实际解释需求（Miller，2019；Bhatt等，2020）。  

### （二）研究目标
1. 提出**以用户为中心的可解释框架**，链接XAI技术与外部利益相关者的决策需求，解决ML模型在金融欺诈检测中的可解释性问题。  
2. 结合集成预测模型（XGBoost）与基于Shapley值的解释框架，实现**准确性与可解释性的统一**，为利益相关者提供局部和全局解释。  

## 二、数据与样本选择
### （一）数据来源与特征
1. **数据集**  
   - 涵盖2007-2020年中国非金融企业的37,502个公司年度观测值，其中432个欺诈样本（经监管机构定罪）和37,070个非欺诈样本。  
   - 特征来自资产负债表、利润表和现金流量表的**原始财务数据**，共包含124个特征（如预付款项、商誉、经营利润等），具体列表见附录。  
2. **数据处理**  
   - 采用XGBoost自动处理缺失值，无需额外数据清洗。  
   - 数据集划分：  
     - 训练集（2007-2017年）、测试集（2018-2020年）；  
     - 训练集进一步分为训练子集（2007-2015年）和验证集（2016-2017年），通过网格搜索优化超参数。  

## 三、研究方法
### （一）外部利益相关者的解释需求
1. **四类利益相关者**（Dechow等，2011）：  
   - **监管者**：需评估模型整体可信度，识别异常样本以深入调查。  
   - **审计师**：需获取可靠审计证据，降低审计风险。  
   - **分析师**：需提升欺诈评估准确性，避免声誉损失。  
   - **投资者**：需筛选低风险企业，规避投资损失。  
2. **解释需求差异**（表1）：  
   - **局部解释**：所有利益相关者均需了解单个样本预测的特征贡献（如“哪些特征导致该企业被判定为欺诈”）；监管者和审计师还需知道模型如何得出预测。  
   - **全局解释**：监管者、审计师、分析师需了解重要特征的影响及特征间交互作用；投资者更关注特征整体影响。  

### （二）提出的方法框架
1. **集成模型构建（图1）**  
   - **步骤1：欠采样训练子集**  
     针对类别不平衡问题，生成多个欠采样子集（每个子集包含全部欺诈样本和随机抽样的非欺诈样本，数量与欺诈样本相等）。  
   - **步骤2：训练基分类器**  
     每个子集训练一个XGBoost分类器，利用其在金融数据中的高效性能。  
   - **步骤3：基分类器集成与筛选**  
     采用多数投票法组合基分类器，最终预测为多数基分类器的结果；筛选与最终预测一致的基分类器作为“正确模型”（集合R）。  
   - **步骤4：基于SHAP值的解释计算**  
     使用Tree SHAP算法（Lundberg等，2020）生成局部解释，解决经典SHAP计算成本高的问题；通过Shapley值公式（式2-3）量化特征贡献。  
   - **步骤5：生成多层次解释**  
     - **局部解释**：计算正确模型的Shapley值均值，反映单个样本的特征贡献方向与大小（如预付款项对观测A的正向贡献最大）。  
     - **全局解释**：对所有样本的Shapley值绝对值取平均，分析特征整体影响及交互作用（如商誉与经营利润的交互影响欺诈概率）。  

## 四、实证分析
### （一）模型性能
1. **预测准确性**  
   - 集成模型AUC为0.79，显著高于单一XGBoost模型（AUC=0.58）及欠采样、过采样等方法（AUC分别为0.76、0.63、0.69），验证了集成方法的有效性。  

### （二）解释结果
1. **局部解释**  
   - **特征贡献差异**：观测A的主要正向特征为“预付款项”（+0.26），观测B为“商誉”（+0.2），体现个体预测的个性化。  
   - **决策过程可视化**：通过决策图对比基分类器的预测差异（如图3(b)中部分基分类器对观测B的预测为0），为利益相关者提供调查线索。  
2. **全局解释**  
   - **重要特征识别**：预付款项、商誉、非控制性权益等是关键欺诈预测特征，其中商誉作为新兴特征与会计文献中“商誉减值操纵”结论一致（Han等，2021；Li & Sloan，2017）。  
   - **特征交互作用**：经营利润的影响随商誉或总资产的不同而变化，表明特征间存在复杂交互（如图6）。  

## 五、结论与展望
### （一）主要结论
1. 提出的**用户中心XAI框架**通过SHAP实现局部与全局解释，满足监管者、审计师等外部利益相关者的差异化需求。  
2. 集成模型结合原始财务数据与XGBoost，在提升预测准确性的同时，通过Shapley值提供可验证的解释（如商誉的重要性），为会计专家提供分析依据。  

### （二）未来研究方向
1. 探索其他XAI技术（如Shapley Lorenz值）与现有框架的对比。  
2. 纳入公平性评估（如Chen等，2022b），扩展模型的伦理维度。  
3. 开发基于预测性能的模型选择程序，优化解释的可靠性。  

## 六、数据与代码可用性
数据可通过请求获取，未提及代码开源情况。  

## 七、关键公式与图表
- **多数投票公式**（式1）：通过基分类器投票确定最终预测。  
- **Shapley值加性模型**（式2-3）：分解模型输出为特征贡献之和。  
- **图表概览**：  
  - 图1：方法框架流程图；  
  - 图2-3：局部解释（特征贡献条形图、决策图）；  
  - 图4-6：全局解释（特征重要性排序、依赖关系散点图）。