
# 《Explanatory artificial intelligence (YAI): human-centered explanations of explainable AI and complex data》总结
- 2024年
## 一、研究背景与核心问题
1. **XAI的局限性**  
   - XAI（可解释人工智能）仅提供基础解释信息，但无法满足用户个性化需求。例如，银行信贷审批系统的XAI可能仅告知贷款被拒原因是“信用查询过多”，却无法解释如何减少查询或区分“硬查询”与“软查询”。  
   - 用户对解释的需求复杂多样（如术语、系统性能、输出原因等），单一XAI无法覆盖所有需求。  
2. **用户中心解释的必要性**  
   - 通用解释（如“一刀切”式说明）要么信息不足，要么过于庞杂。例如，欧盟《人工智能法案》要求高风险系统提供详细技术文档，但直接呈现全部内容会让用户不堪重负。  
   - 解释需结合用户背景、目标和语境，例如调查银行抢劫案时，监控录像的价值取决于调查者的具体问题（如劫匪外貌、逃跑方向等）。  

## 二、YAI的理论框架与核心概念
### （一）YAI的定义与定位
- **定义**：YAI（解释性人工智能）是构建在XAI之上的软件工具，通过**用户中心方法**优化解释质量，根据用户需求动态筛选和呈现信息。  
- **与XAI的区别**：  
  - XAI负责生成可解释信息（如模型决策逻辑），YAI负责从海量信息中选择最相关的解释。  
  - 类比：XAI如同监控录像存储系统，YAI则是高效的视频搜索和播放工具。  

### （二）理论基础：Achinstein的解释理论
- **解释的本质**：解释是一种**言语行为**（illocutionary act），需务实回答问题，不仅涵盖直接答案，还需关联“为何”“如何”“何时”等原型问题（如“硬查询为何影响信用评分”）。  
- **用户中心的关键**：解释需适配用户的知识水平和目标，避免信息过载。例如，向非专家解释信用评分时，需简化技术细节并突出行动建议。  

### （三）解释空间与用户中心属性（SAGE模型）
1. **解释空间（Explanatory Space）**  
   - 定义为问题与答案构成的**超图（Hypergraph）**，包含所有可能的解释路径。例如，解释“心脏疾病风险”可能涉及“年龄”“血压”“胆固醇”等多个关联问题。  
2. **SAGE属性**  
   - **Sourced（来源受限）**：解释需基于特定数据集或系统（如信贷审批模型的规则）。  
   - **Adaptable（可适配）**：根据用户目标调整解释路径，如为高风险用户优先提供改善建议。  
   - **Grounded（基于问答）**：以原型问题（如“为什么”“如何”）驱动解释生成。  
   - **Expandable（可扩展）**：通过语义网络连接不同解释片段，形成连贯的信息网络。  

### （四）解释空间的高效探索（ARS启发式）
- **抽象（Abstraction）**：将解释划分为层次结构（如“信用评分→查询类型→硬查询”），便于用户逐步深入。  
- **相关性（Relevance）**：按用户目标排序信息，如贷款申请者更关注“如何提高审批概率”而非模型技术细节。  
- **简洁性（Simplicity）**：过滤冗余信息，仅展示关键内容（如用摘要替代完整技术文档）。  

## 三、YAI的实现与验证
### （一）YAI4Hu系统：SAGE-ARS模型的实例
- **功能**：  
  - **开放问答（Open Question Answering）**：直接回答用户问题（如“什么是硬查询”）。  
  - **方面概览（Aspect Overviewing）**：通过点击标注术语（如“信用查询”），展示相关原型问题的答案（如“为什么信用查询影响评分”）。  
- **技术特点**：  
  - 从XAI输出或手动文档中提取知识图谱，支持高效检索。  
  - 界面标注术语，用户可通过交互动态扩展解释深度。  

### （二）实证研究
1. **实验设计**  
   - **对比工具**：  
     - **2EC**：提供XAI输出及完整文档（信息过载）。  
     - **How-Why Narrator**：仅支持“如何”“为何”类解释（功能有限）。  
     - **XAI-based Explainer**：提供XAI原始输出（缺乏适配）。  
   - **评估指标**：  
     - **有效性**：用户完成任务的正确率（如回答“如何降低心脏病风险”）。  
     - **满意度**：系统可用性量表（SUS）评分。  
     - **效率**：任务完成时间。  
2. **关键结果**  
   - **YAI4Hu的优势**：在信用审批和心脏病预测场景中，YAI4Hu的有效性和满意度显著高于对比工具。例如，用户回答“如何改善信用评分”的正确率比XAI-based Explainer高30%。  
   - **用户中心设计的重要性**：2EC因信息过载导致用户效率低下，XAI-based Explainer因缺乏灵活性无法满足多样化需求。  

## 四、结论与未来方向
1. **核心结论**  
   - YAI通过用户中心的解释空间分解（如树状结构），显著提升复杂系统的可解释性。  
   - 解释不仅是信息呈现，更是动态适配用户需求的交互过程。  
2. **局限性与未来工作**  
   - **当前不足**：YAI4Hu未完全实现SAGE-ARS模型的自适应机制（如专家用户的深度信息需求）。  
   - **发展方向**：  
     - 扩展至更多XAI类型（如全局解释方法）和应用场景（如教育、机器人）。  
     - 探索YAI在强化学习中的应用，通过解释优化智能体决策。  

## 五、关键词与缩写对照
| 缩写 | 全称 | 中文释义 |
|------|------|----------|
| XAI | Explainable AI | 可解释人工智能 |
| YAI | Explanatory AI | 解释性人工智能 |
| SUS | System Usability Scale | 系统可用性量表 |
| ARS | Abstraction-Relevance-Simplicity | 抽象-相关性-简洁性启发式 |
| SAGE | Sourced-Adaptable-Grounded-Expandable | 来源受限-可适配-基于问答-可扩展属性 |