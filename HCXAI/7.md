# 《OpenHEXAI: An Open-Source Framework for Human-Centered Evaluation of Explainable Machine Learning》主要内容总结
- 2024年
## 摘要
- **背景**：可解释人工智能（XAI）方法的发展对于理解机器学习模型在高风险场景中的行为至关重要。然而，评估这些XAI方法的有效性需要人类受试者的参与，这带来了诸多挑战，如实验设计复杂、可重复性问题以及实施难度大等。
- **目的**：提出OpenHEXAI框架，旨在解决上述挑战，提供一个人性化的XAI方法评估基准。
- **特点**：框架包含多样化基准数据集、预训练模型和事后解释方法的集合；易用的实验研究Web应用；涵盖人类- AI决策任务中事后解释方法有效性的综合评估指标；实验记录的最佳实践建议；以及用于功效分析和成本估算的便捷工具。
- **贡献**：OpenHEXAI是首个大规模的XAI方法基础设施，简化了XAI方法的实验设计和实施，提升了可重复性，并基于此框架对四种先进的事后解释方法进行了系统基准测试。

## 引言
- **XAI的重要性**：在招聘、贷款审批、医疗诊断等高风险应用中，机器学习模型的决策可能显著影响人们的生活，因此提供人类可理解的洞见至关重要。
- **评估XAI的难点**：与客观测量AI预测准确性不同，可解释性难以客观量化，因为它依赖于人类的解释，且目前对客观测量可解释性的指标尚未达成共识。因此，以人为中心的评估（用户研究）常被采用，但设计和开展用户研究复杂且昂贵，导致不同研究结果难以比较。
- **OpenHEXAI的提出**：为了解决上述挑战，提出OpenHEXAI，一个开源的XAI方法评估框架，旨在建立系统化可重复的基准。

## 文献综述
- **研究目的**：通过文献综述了解研究人员在评估XAI方法时的需求，包括用户研究的任务场景、使用的事后解释方法和主要研究问题。
- **任务场景**：研究涵盖了多种用户研究任务场景，如人类-AI决策制定（人类借助AI进行预测）、模型调试和反事实推理等。在人类-AI决策制定中，具体任务包括常识性预测（如新闻分类、收入预测）和需要领域专业知识的预测（如医疗诊断）。
- **事后解释方法**：用户研究涵盖的事后解释方法相对有限，主要集中在SHAP和LIME等方法。
- **主要研究问题**：研究问题多样，包括解释是否提高人类决策准确性、对AI系统工作原理的理解、对AI系统的信任等。

## OpenHEXAI框架
- **基准场景**：框架基于人类-AI联合决策制定场景，这是XAI方法的典型应用场景，且具有明确的需求和约束，便于客观测量XAI方法的有效性。
- **机器学习模块**：负责准备数据实例、模型预测和特征归因分数，为Web应用提供支持。该模块基于OpenXAI库，并增加了额外层，使其适用于用户研究。它支持多种数据集、预训练模型和事后解释方法，并提供了特征语义的代码手册。
- **Web应用模块**：与用户交互的Web应用，包括用户研究任务流程（同意页面、指令和注意力检查、任务阶段、退出调查）。设计考虑了可适应性和可重用性，允许通过配置文件轻松配置不同的数据集和解释方法，无需修改代码。
- **评估模块**：读取用户研究结果并自动计算各种客观评估指标，如准确率、F1、平均时间、过度依赖、公平性指标（AAOD和EOD）等。
- **评估卡**：借鉴数据卡和模型卡的理念，评估卡为记录XAI方法的用户研究设计提供了一个检查清单，包括设计、执行和分析三个阶段的问题。
- **功效分析和成本估算**：基于已进行的基准研究，提供了功效分析参考和成本估算工具，以帮助研究人员更好地规划用户研究的预算。

## 基准研究
- **研究问题**：研究了事后解释方法对人类-AI联合决策的准确性、信任、理解和公平性的影响，具体包括：
  - 效能（RQ1）：事后解释是否提高了人类-AI预测的准确性和/或效率？
  - 信任（RQ2）：事后解释如何影响人类对AI的信任？是否导致过度信任或不信任？
  - 理解（RQ3）：事后解释是否提高了人类对模型工作原理的理解？
  - 公平性（RQ4）：事后解释是否提高了人类-AI联合预测的公平性？
- **实验配置**：在德国信用数据集和RCDV数据集上，对LIME、SHAP、SG和IG四种事后解释方法进行了实验研究。每个数据集有6种用户研究条件（F、FP、FPE-LIME、FPE-SHAP、FPE-SG和FPE-IG），共371名参与者。
- **评估指标**：结合客观指标（如准确率、F1、AAOD、EOD等）和主观调查问题（如对模型的信任、理解等）进行评估。
- **结果**：
  - 客观指标结果表明，事后解释方法（如LIME、SHAP和IG）在德国信用数据集上一般比SG更能提高人类-AI联合性能。在RCDV数据集上，AI辅助对人类-AI联合性能的提高效果不显著。
  - 主观调查结果显示，参与者对LIME解释的信任度最高。LIME和SHAP解释使参与者更好地理解模型的工作原理。

## 限制
- **数据集多样性和数量**：尽管框架纳入了不同领域的数据集，但仍需进一步提高支持的数据集数量和多样性。
- **应用场景和解释方法的局限性**：框架专注于人类-AI决策制定场景和事后解释方法，这可能限制了更广泛XAI方法的评估。
- **评估指标的理解和兼容性**：目前对收集到的客观和主观评估指标的含义和兼容性理解尚浅。
- **评估卡的改进空间**：当前的评估卡实施可能未涵盖所有用户研究设计元素，未来可进一步扩展。

## 结论
- OpenHEXAI框架为XAI方法的人性化评估提供了一个创新的解决方案，简化了用户研究的设计和执行，提高了可重复性。
- 基于OpenHEXAI的基准研究系统评估了四种先进的事后解释方法，揭示了这些方法对人类-AI联合决策效能和公平性以及用户对AI的信任和理解的影响。
- 框架的特点（如多样化的基准数据集、预训练模型、事后解释方法、用户友好的Web应用、综合评估指标和最佳实践指南）有望促进XAI方法人性化评估的广泛应用，并加速该领域的研究进展。