# 《EXPLAIN TO DECIDE: A HUMAN-CENTRIC REVIEW ON THE ROLE OF EXPLAINABLE ARTIFICIAL INTELLIGENCE IN AI-ASSISTED DECISION MAKING》主要内容总结
- 2023年
## 一、引言
- **人工智能发展历程**：自 20 世纪 50 年代起，人工智能历经起伏，近年因硬件进步与数据丰富，在多领域广泛应用。
- **深度学习模型的“黑箱”问题**：以深度神经网络（DNN）为代表的模型性能卓越，但复杂结构使其内部过程难以理解，成为“黑箱”，影响用户信任与应用，尤其在高风险领域。
- **可解释人工智能（XAI）的兴起**：XAI 旨在通过可视化等技术解释模型内部运作，增强用户理解与信任。但目前对解释如何影响人 - AI 决策性能的研究较少。

## 二、应用领域
- **法律领域**：AI 协助法律事务，如法官依据再犯风险评估决定是否保释或量刑。但法律系统中不同角色对 AI 使用方式各异。
- **公共健康领域**：如儿童保护服务评估虐待儿童举报，AI 可辅助决策，错误判断后果严重。
- **内容 moderation 领域**：社交媒体平台面临内容 moderation 挑战，AI 模型用于检测有毒评论和仇恨言论，但存在技术、劳动、民主等多方面问题。

## 三、可解释人工智能（XAI）
- **解释与可解释性的区分**：解释回答“如何”，可解释性回答“为何”。如 Gilpin 等定义解释为回答“为何”的问题，涉及系统内部机制完整描述。
- **XAI 的必要性**：在高风险场景或问题形式化不完整时，如安全、伦理、目标不匹配等情况，需要 XAI 提供解释以增信、促理解和确保公平。
- **可解释性的评估**：Doshi-Velez 和 Kim 提出应用导向、人类导向和功能导向三种评估方法。
- **XAI 的分类**：基于文献的常见分类，包括解释输入输出关系、模型内部表示和透明网络等。

## 四、方法学
- **论文筛选**：通过关键词搜索、参考文献追溯等，从多个学术数据库筛选相关论文，重点关注 2019 年以后包含人类实验的实证研究，最终选定 8 篇论文。

## 五、决策制定
- **相关工作**：Lai 等和 Schemmer 等对人类决策研究进行综述和元分析，指出 XAI 对人类任务性能的提升作用及在文本数据上的有效性。
- **决策模型**：介绍理性模型、快速且节俭模型及基于证据积累的统一模型等心理学决策模型，为理解人类决策过程提供理论基础。

## 六、XAI 与决策制定
- **实证研究综述**：对选定的 8 篇论文进行详细回顾，研究发现 XAI 对人类 - AI 决策性能影响存在差异。
    - **Alufaisan 等（2021）**：AI 预测可提高决策准确性，但解释效果不显著。
    - **Bansal 等（2021）**：解释未显著提高团队性能，且在 AI 错误时提供解释会降低决策准确性。
    - **Carton 等（2020）**：基于特征的解释未提高准确性或一致性，但改变了错误分布，且减少了决策时间。
    - **Lai 等（2023）**：条件委托范式下 XAI 未显著影响性能，全球解释对联合精度有负面影响。
    - **Zhang 等（2020）**：信任校准中，置信度分数影响信任和决策，但解释未显著改善信任校准。
    - **Schemmer 等（2023）**：解释减少不信任，特征重要性对决策制定有积极影响。
    - **Vasconcelos 等（2023）**：解释可减少复杂任务中的过度依赖，成本效益分析有助于理解人类与 AI 的互动。
    - **Leitchmann 等（2023）**：解释提高任务准确性，视觉解释使参与者在模型错误时信任度降低。

## 七、挑战与局限
- **挑战**：包括人类受试者实验的复杂性、人类性能曲线、任务难度、实验设计、解释方法、可视化技术等。
- **局限**：综述仅涵盖 2019 年以后的部分论文，且仅关注实证研究，对理论研究涉及较少。

## 八、结论与未来方向
- **结论**：高性能深度神经网络广泛应用，但复杂性降低透明度，影响用户信任。高风险任务中，人类 - AI 决策制定涉及多学科概念，需深入研究以实现互补团队性能。
- **未来方向**：包括解释方法的组合、不同任务难度下的性能测试、可视化技术对团队性能的影响等。

## 九、参考文献
列出文章引用的参考文献，涵盖机器学习算法、人工智能在各领域的应用、可解释人工智能的方法与挑战、人类 - AI 决策制定研究等方面。