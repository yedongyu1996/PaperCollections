# Toward Human-centered XAI in Practice: A survey
-2024年
## Abstract
本文综述了可解释人工智能（XAI）技术的发展，指出AI系统的复杂性和不透明性阻碍了人类的采用。尽管已开发出多种XAI方法和工具，但目前XAI技术的应用仍缺乏以“人”为中心的指导，无法满足不同利益相关者的需求。文章首先总结了一个以人为核心的XAI需求框架，将不同利益相关者划分为五种关键角色，并提出了六种常用的人类中心XAI评估措施。此外，文章还开发了一个针对视觉计算的XAI方法分类体系，并分析了方法的属性。最后，以医学影像诊断为例，展示如何将现有XAI方法应用于满足不同利益相关者的实际需求，并检查了开源XAI工具的可用性。

## 1. Introduction
随着AI技术在医疗诊断、金融贷款、自动驾驶和司法决策等领域的广泛应用，AI的“黑箱”问题变得尤为突出。XAI技术旨在提升用户信任、促进人机协作并减少算法偏见。欧盟的GDPR进一步强调了用户对解释的权利。然而，早期XAI研究多从技术角度出发，缺乏对人类用户多样化需求的关注。文章强调，XAI研究需要转向更“以人为本”的方式，通过利益相关者的需求驱动XAI技术的设计和选择。

## 2. Human-centered demands and evaluation for XAI
### 2.1 Background and motivation
自DARPA提出XAI项目以来，XAI概念被广泛接受。文章区分了“可解释性”和“可理解性”，前者指模型向人类提供解释的能力，后者指模型本身能被人类理解的能力。XAI挑战在于将技术设计与人类需求结合，特别是不同利益相关者的多样化需求。

### 2.2 Human-centered demand framework
文章提出了一个基于角色的人类中心需求框架，将利益相关者分为五种关键角色：模型开发者、最终用户、管理决策者、影响群体和监管机构。每个角色的需求如下：
- **模型开发者**：理解模型内部机制、追踪模型错误或偏差以改进模型。
- **最终用户**：了解预测的不确定性并校准信任；利用领域知识审计预测逻辑；增强专业知识以支持未来决策。
- **管理决策者**：检查模型机制以验证AI有效性；更新领域知识以支持管理决策。
- **影响群体**：从个人视角了解影响原因；确保数据正确使用和隐私保护。
- **监管机构**：审计模型在可靠性、公平性、隐私和伦理方面的问题。

### 2.3 Human-centered XAI evaluation
文章提出六种人类中心XAI评估措施，包括理解或心理模型、信任、有用性、性能提升、伦理（公平性、隐私）和满意度。这些措施旨在评估XAI系统是否真正满足用户的解释需求。

## 3. XAI methodology for visual computing
文章将视觉计算领域的XAI方法分为四类：视觉解释、基于实例的解释、基于知识的解释和因果解释。每种方法都有其优缺点：
- **视觉解释**：通过梯度、CAM、注意力机制等方法直接解释输入图像，但存在解释效率低、缺乏因果和语义内容的问题。
- **基于实例的解释**：通过选择数据集中的实例提供解释，但需要人类解释且不提供重要区域。
- **基于知识的解释**：结合模型内部或外部的语义知识，但提取的语义信息可能难以理解。
- **因果解释**：通过因果发现、干预和反事实方法提供因果信息，但实现复杂且需要额外的因果推理方法。

## 4. Match human-centered demands with XAI methods in medical diagnosis
文章以医学影像诊断为例，展示了如何将人类中心需求框架应用于特定领域，为不同利益相关者推荐合适的XAI方法。例如：
- **模型开发者**：可使用梯度、CAM等视觉解释方法，知识蒸馏等基于知识的解释方法，以及反事实解释等因果解释方法。
- **最终用户**：如医生，可使用梯度-CAM等视觉解释方法，LIME等基于实例的解释方法，以及TCAV等基于知识的解释方法。
- **管理决策者**：可使用原型等基于实例的解释方法，以及基于知识的解释方法。
- **影响群体**：如患者，可使用CAM等视觉解释方法，知识蒸馏等基于知识的解释方法，以及基于实例的解释方法。
- **监管机构**：可使用梯度、CAM等视觉解释方法，TCAV等基于知识的解释方法，以及反事实解释等因果解释方法。

## 5. XAI toolbox for stakeholders
文章总结了现有的XAI工具，分析了它们的支持框架、提供的解释方法类型以及满足的不同群体需求。例如：
- **SHAP**：支持TensorFlow和PyTorch，提供多种视觉解释方法。
- **Lucid**：旨在帮助模型开发者调试模型，提供与隐藏神经元可视化相关的解释。
- **AIX360**：不仅满足模型开发者的需求，还为其他群体提供特定的人格化解释和教育资源。
- **Thales XAI平台**：结合机器学习和推理能力，提供视觉、基于实例和反事实解释方法，满足最终用户和影响群体的需求。

## 6. Challenges and future works
尽管XAI取得了一定进展，但仍存在诸多挑战，如某些XAI方法可能提供不可靠的解释，缺乏因果推理设计和验证方法，以及技术中心而非以人为中心的解释。此外，XAI工具箱不足，缺乏对透明度和公平性的评估。未来工作需要进一步验证和完善人类中心需求框架，开发更具体的人类中心XAI评估指标和方法，并在更多应用场景中进行调查和总结。

## 7. Conclusions
本文通过综述XAI的最新研究进展，从利益相关者需求、方法和工具、应用等角度，为理论与实践的结合提供了指导。文章提出的框架和方法映射为从业者提供了确定利益相关者实际需求的指导，特别是在医学影像场景中的应用。